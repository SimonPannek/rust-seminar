% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\section{Asynchronous networking using Tokio}
In the chapter before basic examples for TCP and UDP communication in Rust were shown. In those programs the current
thread was blocked when waiting for new connections or incoming messages. This not only results into the program having
to wait for a new message to continue executing the code, it also means that you have to create a new thread for each
connection, if you want to listen to multiple connections at once. This principle is called sequential programming. In
contrast to that there are no blocking functions in asynchronous programming. Instead results are wrapped in so called
futures which represent a result of an otherwise blocking function. This result can be unwrapped when it is actually
needed instead of waiting for the execution in advance. \cite{c18network}

\subsection{The tokio crate}
The core library of Rust is kept very small. Instead it offers a package manager called cargo for easy import of third
party packages. This way those extensions are not bound to the release cycle of Rust and also breaking changes will just
affect one single package instead of the whole library. For compatibility reasons a project can keep using an older
version of a package while upgrading the standard library. The packages of Rust are called crates.

A crate is similar to a binary or library. You can add the name and the version to the dependencies of a project in
order to import it. If you do not want to compile and include a whole library, some crates also offer flags deciding
which parts of the library should be included.

To introduce asynchronous programming to Rust, you usually want to use a crate which enables async methods. In this
chapter the crate "tokio" is used. As the documentation \cite{tokio-doc} states, tokio is a runtime for writing
reliable network applications. It offers tools for working with asynchronous tasks as well as APIs for typical blocking
operations suchs as sockets and filesystem operations.

A task in the context of a tokio program is a non-blocking piece of code. You can use the \rust{tokio::task::spawn}
method to schedule the task on the Tokio runtime and use \rust{await} to wait for the returned value. This is a typical
behavior in asynchronous programming and you can compare a task to a promise in programming languages like JavaScript.

The tokio runtime consists out of three major parts: An I/O event loop, which handles I/O events and dispatches them to
the tasks which are waiting for them, a timer to run tasks after a certain period of time and  a scheduler, which
actually executes the tasks on the different threads. To enable the tokio runtime, you can mark the main function with
the \rust{async} keyword and add the \mintinline{rust}{#[tokio::main]} macro above.

The scheduler swaps around all tasks which need to be run. This means there can be more tasks than threads whilst still
allowing concurrent execution of all tasks. To tell the scheduler that the program is currently waiting for another
task to finish before it can continue running, you can call \rust{await} on the method. This effectively stops the
execution of the current task, waits for the other task to finish and then unwraps the returned result. Contrary to an
execution without the tokio runtime, the scheduler knows when the current task can continue running and uses the
otherwise unused CPU time by letting another task run instead.

\subsection{Programming with futures}
Let us now take a look at the small demonstration of futures in figure 6:

\begin{figure}[h]
    \begin{minted}{Rust}
async fn async_function() {
    println!("Started task1");
    sleep(Duration::from_secs(5))
        .await;
    println!("Finished task1");
}

#[tokio::main]
async fn main() {
    let future1 = async_function();

    let future2 = async {
        println!("Started task2");
        sleep(Duration::from_secs(3))
            .await;
        println!("Finished task2");
    };

    '\textcolor{macro}{join!}'(future1, future2);
}
    \end{minted}
    \caption{Future demonstration}
\end{figure}

The declared types of functions play an important role when working with tokio. This is the reason why the example
above also includes the entire function definitions.

As you can see, the main function is marked as \rust{async} and the tokio macro is used to create a runtime for our
program to run in. Next, the asynchronous function \rust{async_function} is called. The value returned by the function
has the type \rust{std::future::Future}, which wraps the value going to be returned after the computation. The
\rust{Future} is saved in the variable \rust{future1}. This way the tokio runtime actually decides when to evaluate the
function. If \rust{async_function} was not marked as an asynchronous function, the main thread executing the program
would enter the function, completely evaluate it and after that continue executing the main function.

The type of \rust{future2} is also a \rust{Future}, the only difference being that it uses an \rust{async} block
instead of an extra function to define the asynchronous code.

In the end the macro \nohighlight{join!} is used to wait for both futures to evaluate completely while executing them
concurrently.

Both functions first print a short message, that they have started executing, then they sleep a certain amount of time
using \rust{tokio::time::sleep} and then print that they have stopped executing. The key part in this is, that the
function wrapped by \rust{future1} is called first and sleeps for 5 seconds, the function wrapped by \rust{future2} is
called afterwards and only sleep for three.

In a blocking environment it would result the program to take more than eight seconds to execute, because first the
returned value of \rust{future1} is evaluated and only after that the value of \rust{future2}. This way, the output
would look like this:

\begin{minted}[frame=lines]{TeX}
Started task1
Finished task1
Started task2
Finished task2
\end{minted}

The only way to avoid the synchronous execution is by explicitly starting two threads and executing both tasks
concurrently. If multiple tasks have to run in parallel and include a lot of blocking operations, you usually do not
want to create a thread for each task as the creation of tasks adds a lot of overhead to the program.

In an asynchronous environment on the other hand, the scheduler swaps the execution when the program reaches an
\rust{await} point. This way, the sleep function does not block the entire program flow and the function wrapped by
\rust{future2} finishes first, even if only executing on one thread. The resulting output of the program looks like
this:

\begin{minted}[frame=lines]{TeX}
Started task1
Started task2
Finished task2
Finished task1
\end{minted}

\subsection{Asynchronous TCP communication}
Tokio offers the module \rust{tokio::net}, which contains a TCP and UDP networking API similar to the one of the
standard library used in chapter 3. Those functions are marked as \rust{async}, blending in with the rest of the tokio
ecosystem.

Let us first use tokio to spawn a new task which handles a connection by reading all incoming messages and printing the
output to the command line. The code of figure 7 implements basic asynchronous client handeling:

\begin{figure}[h]
    \begin{minted}{Rust}
tokio::spawn(async move {
    let mut reader =*BufReader::new(&mut stream);

    loop {
        let mut buf = String::new();

        match reader
            .read_line(&mut buf).await {
            Ok(0) => break,
            Ok(size) =>*print!("{}", buf),
            Err(e) => {
                eprintln!("{}", e);
                stream
                    .shutdown().await?;
                break;
            }
        }
    }

    Ok::<(), Error>(())
});
    \end{minted}
    \caption{Asynchronous TCP client handeling}
\end{figure}

The current connection is represented as the variable \rust{stream} which has the type \rust{tokio::net::TcpStream}.
The function \rust{tokio::spawn} takes a \rust{Future} as an argument and creates a new asynchronous task. When and how
this task gets executed depends on the configuration of the current runtime.

As an argument the function is given an asynchronous code block, which returns a \rust{Future} as seen in figure 6.
Here additionaly to the \rust{async} keyword, \rust{move} is used. This means the asynchronous block will take the
ownership of all variables referenced within it. Without this keyword, all variables would be bound to the scope of the
code surrounding it. This is especially important as our asynchronous code needs full ownership of the \rust{stream}
variable in order to read from it and shut it down in case of an error.

The rest of the code is similar to the synchronous TCP client handeling of figure 1: In \textcolor{orange}{line 3} a
buffered reader instance is created for more efficient reading from the \rust{TcpStream}. After that the program enters
a loop to repeatedly read the input of the connection. In \textcolor{orange}{line 5} a mutable \rust{String} named
\rust{buf} is created to save the incoming data. Then the method \rust{read_line} of \rust{reader} is called with
\rust{buf} as an argument. As the reader instance has the type \rust{tokio::io::BufReader} in this asynchronous
implementation, \rust{read_line} does not return a simple \rust{Result} but rather a \rust{Future} which wraps around
the returned value. This means we can use an \rust{await} point to tell the scheduler it can stop running the current
program until new data has arrived.

If reading was successful and no bytes were read, it means that the connection was closed and we can break out of the
loop. If at least one byte was read the program enters the second match arm and the read data gets printed to the
command line.

If reading has failed, the program enters the third match arm and prints the error to the command line. After that it
tries to shutdown the stream. Here the \rust{shutdown} method writes all buffered data to the stream and shuts down the
write instance of the stream using a sys call. Other than the \rust{shutdown} method of \rust{std::io::TcpStream}, it
does not take an enum as an argument to control how the stream should be shut down. The method also returns an
\rust{Result} wrapped by a \rust{Future}. First the future gets unwrapped by calling \rust{await} on it. Afterwards the
result is unwrapped using the \rust{?} operator and the program breaks out of the loop.

In the end of the task we have to explicitly specify the type of the result returned. The reason for this is the use of
the \rust{?} operator in \textcolor{orange}{line 14} to unwrap the returned value of the \rust{shutdown} method. As
\rust{async} blocks do not have a explicitly specified return type, the compiler sometimes fails to infer the error
type of the async block. This will trigger the compiler error \nohighlight{type annotations needed}. By specifying the
exact type of the \rust{Ok} enum variant, the compiler is able to infer the return type and we are able to use the
\rust{?} operator. Another way to handle this would be to use the \rust{expect} method instead or create an extra
asynchronous function instead of using \rust{async} blocks. \cite{async-rust}

As in chapter 3 we can now wrap our code from figure 7 into a function called \rust{handle_client} to be able to call
it in figure 8. Next we are going to open the TCP streams and call the \rust{handle_client} with the stream as an
argument. It was already mentioned before that having TCP connections to multiple clients would require a thread for
each connection. Tokio solves this problem as the event loop manages and listens to every connection. Instead of
creating a thread for each connection it is only required to spawn a task for each TCP stream. That is why the
following code examples supports connecting to multiple clients:

\begin{figure}[h]
    \begin{minted}{Rust}
let listener = TcpListener::bind(ADDRESS).await?;

loop {
    let (stream, _) = listener.accept().await?;

    handle_client(stream);
}
    \end{minted}
    \caption{Asynchronous TCP listener}
\end{figure}

First a \rust{tokio::net::TcpListener} is created by binding it to the address saved in the \rust{&str} constant
\rust{ADDRESS}. This value corresponds to the ip address and port the server should listen to. The future result is
then unwrapped using \rust{await} as well as \rust{?} and saved to the variable \rust{listener}. As the
\rust{TcpListener} of tokio does not implement the \rust{incoming} method, we have to call the \rust{accept} method of
the listener ourselves. This method again returns a tupel including the \rust{TcpStream} and the address of the new
connection, wrapped by a \rust{Result} to handle errors and a \rust{Future} to make the waiting operation non-blocking.
After that the received stream can be given to our \rust{handle_client} function, which spawns the new task handeling
the client. As the client handeling runs in an extra task, the program can continue to run and accept new connections
without having to wait for the current one to disconnect.

To connect to this server one could just use the TCP client of figure 3, as both servers use the same protocol and can
work together interchangeably. To showcase how the tokio library affects a client implementation, there is a smaller
demonstration of an asynchronous TCP client in figure 9.

\begin{figure}[h]
    \begin{minted}{Rust}
let mut stream = TcpStream::connect(ADDRESS).await?;

let mut reader = BufReader::new(stdin());

loop {
    let mut buf = String::new();

    match reader
        .read_line(&mut buf).await {
        Ok(_) => {
            let bytes = buf.as_bytes();

            stream
                .write_all(bytes)
                .await?;
        }
        Err(e) => {
            eprintln!("{}", e);
            stream.shutdown().await?;
            break;
        }
    }
}
    \end{minted}
    \caption{Asynchronous TCP client}
\end{figure}

In the beginning a \rust{tokio::net::TcpStream} instance is created by connecting to \rust{ADDRESS}, a constant in
which the address of the server is saved as a \rust{&str}. Other than the synchronous implementation, we are not going
to read from the command line directly but rather wrap it using a \rust{tokio::io::BufReader} to be able to read from
it without blocking the task.

The loop for reading messages on the command line and sending them to the server is also very similar to the
synchronous implementation of figure 3: In \textcolor{orange}{line 6} a new \rust{String} called \rust{buf} is created
to save the input of the user. Then the \rust{read_line} method with \rust{buf} as an argument is invoked. As this is
an asynchronous method, we can use the \rust{await} keyword to give up the computing resources until new input was
written into the buffer. In case new input was written to the buffer and the scheduler continues to execute the task,
the return value of \rust{read_line} is matched.

If reading was successful, the program enters the first match arm. In figure 3 this match
arm included a check whether the read string is equal to "exit" to offer a way to close the connection other than just
closing it forcefully. This part is removed in figure 9 as it works the same way. The read input is converted to bytes
using \rust{as_bytes} and saved in the variable \rust{bytes} and after that the byte slice is given as an argument to
the \rust{write_all} method of \rust{stream}. The returned \rust{Future} is again unwrapped using \rust{await} and the
contained \rust{Result} unwrapped using \rust{?}.

If reading has failed the error is printed to the error output, the program tries to shutdown the stream using its
\rust{shutdown} method, unwrap the \rust{Future} and \rust{Result} using \rust{await} and \rust{?} and then break out
of the loop.

When running the TCP client, the asynchronous client does not differ as much from the synchronous client as the server
implementations differ from each other. The server implementation can use the scheduling from tokio to handle multiple
connections at once. The client on the otherhand is idle anyway, when it is waiting for input on the command line,
because it does not have any additional tasks. If you expanded the implementation to a complete chat client which can
both send and receive message, a second task would be needed to listen for new data on the connection. Here an
asynchronous environment would be of more use.

\subsection{Asynchronous UDP communication}
When it comes to asynchronous UDP communication, tokio offers the struct \rust{tokio::net::UdpSocket} which works
similar to the \rust{UdpSocket} of the standard library used in figure 4 and 5 for the UDP sender and receiver.

Porting the synchronous UDP implementation to an asynchronous one works the same as we have done for the TCP client and
server in the previous subchapter: Blocking operations are replaced by non-blocking operations returning futures. If
the current task has to wait for a future to evaluate, \rust{async} can be used.

As UDP does not implement actual connections between the sender and the receiver and only binds to an address, the
behavior of the asynchronous UDP implementation will also not differ as much from the synchronous example, similar to
the example of figure 9. At least this is the case if the sender and receiver are as strictly seperated from each other
as they are in the synchronous UDP example. Otherwise the struct \rust{std::sync::Arc} could be used to share the
ownership of the \rust{UdpSocket} to have a sending and receiving task in the same client. This is possible, because
both the \rust{send_to} and \rust{recv_from} methods of the socket do not require a mutable reference to the socket. If
this was not the case, you could not share one socket for two tasks as there can only be one mutable reference to a
certain piece of data in Rust.
